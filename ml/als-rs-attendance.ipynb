{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"ALS\").config(\"spark.driver.host\",\"localhost\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(уникальный номер занятия=401346550, уникальный номер группы=801346550, уникальный номер участника=101352023, направление 2='ОНЛАЙН Гимнастика', направление 3='ОНЛАЙН Цигун', онлайн/офлайн='Да', дата занятия='2022-08-01', время начала занятия='09:00:00', время окончания занятия='10:00:00'),\n",
       " Row(уникальный номер занятия=401346550, уникальный номер группы=801346550, уникальный номер участника=101385462, направление 2='ОНЛАЙН Гимнастика', направление 3='ОНЛАЙН Цигун', онлайн/офлайн='Да', дата занятия='2022-08-01', время начала занятия='09:00:00', время окончания занятия='10:00:00'),\n",
       " Row(уникальный номер занятия=401346550, уникальный номер группы=801346550, уникальный номер участника=101421897, направление 2='ОНЛАЙН Гимнастика', направление 3='ОНЛАЙН Цигун', онлайн/офлайн='Да', дата занятия='2022-08-01', время начала занятия='09:00:00', время окончания занятия='10:00:00'),\n",
       " Row(уникальный номер занятия=401346550, уникальный номер группы=801346550, уникальный номер участника=101354499, направление 2='ОНЛАЙН Гимнастика', направление 3='ОНЛАЙН Цигун', онлайн/офлайн='Да', дата занятия='2022-08-01', время начала занятия='09:00:00', время окончания занятия='10:00:00'),\n",
       " Row(уникальный номер занятия=401346550, уникальный номер группы=801346550, уникальный номер участника=101421312, направление 2='ОНЛАЙН Гимнастика', направление 3='ОНЛАЙН Цигун', онлайн/офлайн='Да', дата занятия='2022-08-01', время начала занятия='09:00:00', время окончания занятия='10:00:00')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the dataset into pyspark DataFrame\n",
    "attendance = spark.read.csv('./attend.csv', header='true', inferSchema = 'true')\n",
    "attendance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('уникальный номер занятия', 'int'),\n",
       " ('уникальный номер группы', 'int'),\n",
       " ('уникальный номер участника', 'int'),\n",
       " ('направление 2', 'string'),\n",
       " ('направление 3', 'string'),\n",
       " ('онлайн/офлайн', 'string'),\n",
       " ('дата занятия', 'string'),\n",
       " ('время начала занятия', 'string'),\n",
       " ('время окончания занятия', 'string')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attendance.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't going to need the time stamp, so we can go ahead and remove that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('уникальный номер группы', 'int'),\n",
       " ('уникальный номер участника', 'int'),\n",
       " ('онлайн/офлайн', 'string')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attendance = attendance.drop('дата занятия').drop('время начала занятия').drop('время окончания занятия').drop('направление 2').drop('направление 3').drop('уникальный номер занятия')\n",
    "attendance.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance = attendance.withColumn(\"rating\",\n",
    "                     when((attendance['онлайн/офлайн'] == \"Да\"), 1).\n",
    "                     when((attendance['онлайн/офлайн'] == \"Нет\"), 2)\n",
    "                     .otherwise(lit(\"0\"))).drop('онлайн/офлайн')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(уникальный номер группы=801346550, уникальный номер участника=101352023, rating=1),\n",
       " Row(уникальный номер группы=801346550, уникальный номер участника=101385462, rating=1),\n",
       " Row(уникальный номер группы=801346550, уникальный номер участника=101421897, rating=1),\n",
       " Row(уникальный номер группы=801346550, уникальный номер участника=101354499, rating=1),\n",
       " Row(уникальный номер группы=801346550, уникальный номер участника=101421312, rating=1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attendance = attendance.withColumn(\"rating\",attendance.rating.cast(IntegerType()))\n",
    "attendance.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Alternating Least Squares Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this dataset is already preprocessed for us, we can go ahead and fit the Alternating Least Squares model.\n",
    "\n",
    "* Import the ALS module from pyspark.ml.recommendation.\n",
    "* Use the randomSplit method on the pyspark DataFrame to separate the dataset into a training and test set\n",
    "* Fit the Alternating Least Squares Model to the training dataset. Make sure to set the userCol, itemCol, and ratingCol to the appropriate names given this dataset. Then fit the data to the training set and assign it to a variable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "# split into \n",
    "(training, test) = attendance.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5,rank=4, regParam=0.01, userCol=\"уникальный номер участника\", itemCol=\"уникальный номер группы\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# fit the ALS model to the training set\n",
    "model = als.fit(training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've fit the model, and it's time to evaluate it to determine just how well it performed.\n",
    "\n",
    "* import the RegressionEvalutor from pyspark.ml.evaluation\n",
    "* generate predictions with your model for the test set by using the `transform` method on your ALS model\n",
    "* evaluate your model and print out the RMSE from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.32954920535419624\n"
     ]
    }
   ],
   "source": [
    "# importing appropriate library\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation to Find the Optimal Model\n",
    "\n",
    "Let's now find the optimal values for the parameters of the ALS model. Use the built-in Cross Validator in pyspark with a suitable param grid and determine the optimal model. Try with the parameters:\n",
    "\n",
    "* regularization = [0.01,0.001,0.1])\n",
    "* rank = [4,10,50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/22 23:55:06 WARN BlockManager: Block rdd_247_0 already exists on this machine; not re-adding it\n",
      "23/05/22 23:55:06 WARN BlockManager: Block rdd_247_0 already exists on this machine; not re-adding it\n",
      "23/05/22 23:55:06 WARN BlockManager: Block rdd_247_0 already exists on this machine; not re-adding it\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "als_model =  ALS(userCol=\"уникальный номер участника\", itemCol=\"уникальный номер группы\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "                 \n",
    "params = ParamGridBuilder().addGrid(als_model.regParam, [0.01,0.001,0.1]).addGrid(als_model.rank, [4,10,50]).build()\n",
    "\n",
    "\n",
    "## instantiating crossvalidator estimator\n",
    "cv = CrossValidator(estimator=als_model, estimatorParamMaps=params,evaluator=evaluator,parallelism=4)\n",
    "best_model = cv.fit(attendance)    \n",
    "\n",
    "# We see the best model has a rank of 50, so we will use that in our future models with this dataset\n",
    "best_model.bestModel.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_807e18f91ad4, rank=50"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Alternating Least Squares Model with rank 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "# split into \n",
    "(training, test) = attendance.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5,rank=50, regParam=0.01, userCol=\"уникальный номер участника\", itemCol=\"уникальный номер группы\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# fit the ALS model to the training set\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3258:============================>                           (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.06521803776431001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3258:=================================================>      (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# importing appropriate library\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Recommendations\n",
    "\n",
    "Now it's time to actually get some recommendations! The ALS model has built in methods called `recommendForUserSubset` and `recommendForAllUsers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next line, we are creating an RDD with the top 10 recommendations for every user and then selecting one user to find out his predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(уникальный номер участника=101352023, recommendations=[Row(уникальный номер группы=801368559, rating=2.6092336177825928), Row(уникальный номер группы=801361070, rating=2.6085448265075684), Row(уникальный номер группы=801349882, rating=2.598538398742676), Row(уникальный номер группы=801367536, rating=2.464735984802246), Row(уникальный номер группы=801367541, rating=2.397475004196167), Row(уникальный номер группы=801352724, rating=2.395500659942627), Row(уникальный номер группы=801361139, rating=2.3931448459625244), Row(уникальный номер группы=801371881, rating=2.3717169761657715), Row(уникальный номер группы=801359089, rating=2.3572616577148438), Row(уникальный номер группы=801349262, rating=2.341343402862549)])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations = model.recommendForAllUsers(10)\n",
    "recommendations.where(recommendations[\"уникальный номер участника\"] == 101352023).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
