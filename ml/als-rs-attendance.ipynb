{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"ALS\").config(\"spark.driver.host\",\"localhost\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9458:>                                                       (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('groupId', 'int'), ('userId', 'int'), ('rating', 'int')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(groupId=801346550, userId=101352023, rating=1),\n",
       " Row(groupId=801346550, userId=101385462, rating=1),\n",
       " Row(groupId=801346550, userId=101421897, rating=1),\n",
       " Row(groupId=801346550, userId=101354499, rating=1),\n",
       " Row(groupId=801346550, userId=101421312, rating=1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the dataset into pyspark DataFrame\n",
    "attendance = spark.read.csv('./attend.csv', header='true', inferSchema = 'true')\n",
    "attendance = attendance.drop('дата занятия') \\\n",
    "                        .drop('время начала занятия') \\\n",
    "                        .drop('время окончания занятия') \\\n",
    "                        .drop('направление 2') \\\n",
    "                        .drop('направление 3') \\\n",
    "                        .drop('уникальный номер занятия') \\\n",
    "                        .withColumnRenamed('уникальный номер группы', 'groupId') \\\n",
    "                        .withColumnRenamed('уникальный номер участника', 'userId') \\\n",
    "                        .withColumn(\"rating\", when((attendance['онлайн/офлайн'] == \"Да\"), 1) \\\n",
    "                                                .when((attendance['онлайн/офлайн'] == \"Нет\"), 1) \\\n",
    "                                                .otherwise(lit(\"0\"))) \\\n",
    "                        .drop('онлайн/офлайн')\n",
    "attendance = attendance.withColumn(\"rating\", col('rating').cast(IntegerType()))\n",
    "print(attendance.dtypes)\n",
    "attendance.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Alternating Least Squares Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this dataset is already preprocessed for us, we can go ahead and fit the Alternating Least Squares model.\n",
    "\n",
    "* Import the ALS module from pyspark.ml.recommendation.\n",
    "* Use the randomSplit method on the pyspark DataFrame to separate the dataset into a training and test set\n",
    "* Fit the Alternating Least Squares Model to the training dataset. Make sure to set the userCol, itemCol, and ratingCol to the appropriate names given this dataset. Then fit the data to the training set and assign it to a variable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/25 23:36:40 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/25 23:36:40 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/25 23:36:40 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "# split into \n",
    "(training, test) = attendance.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5,rank=4, regParam=0.01, userCol=\"userId\", itemCol=\"groupId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# fit the ALS model to the training set\n",
    "model = als.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=101346610, features=[-0.09497742354869843, 0.057440996170043945, -0.00277847982943058, 0.15691901743412018, -0.08705680072307587, 0.06693026423454285, 0.14026832580566406, 0.12499383836984634, 0.045402467250823975, 0.025674080476164818, -0.013423117808997631, 0.06911619752645493, -0.20548807084560394, -0.04170578718185425, -0.12842290103435516, 0.01386646181344986, -0.036058805882930756, 0.06587745994329453, 0.008388432674109936, -0.05730828642845154, -0.10669739544391632, 0.0077640600502491, -0.05352290719747543, 0.09031370282173157, -0.12390410900115967, -0.008904114365577698, 0.005484557244926691, 0.02336527593433857, 0.016564112156629562, -0.04919910430908203, -0.0017062033293768764, 0.04292133077979088, -0.08697185665369034, -0.10933773964643478, 0.0794006884098053, 0.025934170931577682, -0.056310731917619705, 0.15212248265743256, -0.12254568934440613, -0.06573570519685745, -0.04081561788916588, 0.11978831142187119, -0.01604308746755123, 0.001383399241603911, 0.00039014406502246857, 0.060811012983322144, 0.1356506496667862, 0.06751678138971329, 0.06420908868312836, 0.02288779430091381])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.userFactors.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrovnyagin/opt/anaconda3/envs/python38/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id=801346550, features=[-0.5429868698120117, -0.5055123567581177, 0.026591012254357338, -0.23246563971042633, 0.2155381590127945, 0.29472947120666504, -0.06582456827163696, -0.27172520756721497, 0.17953981459140778, 0.15475395321846008, -0.17308685183525085, 0.4240971803665161, -0.020962323993444443, 0.07267415523529053, -0.5862930417060852, 0.278379887342453, 0.12836910784244537, -0.09696116298437119, 0.15661607682704926, 0.3113566040992737, 0.13550250232219696, -0.06602007150650024, 0.0677306205034256, 0.40290719270706177, 0.011147268116474152, 0.03184487298130989, -0.06647253781557083, 0.195112407207489, 0.4735880196094513, -0.41877150535583496, 0.05611487105488777, 0.5212815999984741, 0.18407407402992249, -0.0915948897600174, -0.07038358598947525, 0.37637531757354736, -0.18032342195510864, 0.1895064264535904, -0.11977829784154892, -0.07220704853534698, -0.3280166983604431, -0.026818908751010895, 0.10839580744504929, -0.029526978731155396, 0.04546525329351425, -0.2077278047800064, 0.16057072579860687, 0.10574579983949661, 0.280724436044693, 0.2549462616443634])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.itemFactors.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've fit the model, and it's time to evaluate it to determine just how well it performed.\n",
    "\n",
    "* import the RegressionEvalutor from pyspark.ml.evaluation\n",
    "* generate predictions with your model for the test set by using the `transform` method on your ALS model\n",
    "* evaluate your model and print out the RMSE from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.3900732774893743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# importing appropriate library\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation to Find the Optimal Model\n",
    "\n",
    "Let's now find the optimal values for the parameters of the ALS model. Use the built-in Cross Validator in pyspark with a suitable param grid and determine the optimal model. Try with the parameters:\n",
    "\n",
    "* regularization = [0.01,0.001,0.1])\n",
    "* rank = [4,10,50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_0764f29c0fac, rank=50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "als_model =  ALS(userCol=\"userId\", itemCol=\"groupId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "                 \n",
    "params = ParamGridBuilder().addGrid(als_model.regParam, [0.01,0.001,0.1]).addGrid(als_model.rank, [4,10,50]).build()\n",
    "\n",
    "\n",
    "## instantiating crossvalidator estimator\n",
    "cv = CrossValidator(estimator=als_model, estimatorParamMaps=params,evaluator=evaluator,parallelism=4)\n",
    "best_model = cv.fit(attendance)    \n",
    "\n",
    "# We see the best model has a rank of 50, so we will use that in our future models with this dataset\n",
    "best_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Alternating Least Squares Model with rank 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "# split into \n",
    "(training, test) = attendance.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5,rank=50, regParam=0.01, userCol=\"userId\", itemCol=\"groupId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# fit the ALS model to the training set\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5698:==============>                                         (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.06775719589157905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5698:=================================================>      (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# importing appropriate library\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Recommendations\n",
    "\n",
    "Now it's time to actually get some recommendations! The ALS model has built in methods called `recommendForUserSubset` and `recommendForAllUsers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next line, we are creating an RDD with the top 10 recommendations for every user and then selecting one user to find out his predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userId=101352023, recommendations=[Row(groupId=801369579, rating=2.3925416469573975), Row(groupId=801350039, rating=2.3474323749542236), Row(groupId=801348610, rating=2.3401365280151367), Row(groupId=801365340, rating=2.3213629722595215), Row(groupId=801349631, rating=2.2758753299713135), Row(groupId=801371096, rating=2.2596638202667236), Row(groupId=801349185, rating=2.209477424621582), Row(groupId=801359965, rating=2.20817494392395), Row(groupId=801366418, rating=2.1873857975006104), Row(groupId=801368424, rating=2.1793465614318848)])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations = model.recommendForAllUsers(10)\n",
    "recommendations.where(recommendations[\"userId\"] == 101352023).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User interests (categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = spark.read.csv('./dict.csv', sep =';', header='true', inferSchema = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = categories.drop('Разметка: Для ума/ Для души / Для тела') \\\n",
    "                        .drop('id_level1') \\\n",
    "                        .drop('level1') \\\n",
    "                        .drop('id_level2') \\\n",
    "                        .drop('level2') \\\n",
    "                        .drop('d_level1') \\\n",
    "                        .drop('d_level2') \\\n",
    "                        .drop('d_level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_level3=1042, leve3='Иные интеллектуальные игры'),\n",
       " Row(id_level3=1040, leve3='Викторины'),\n",
       " Row(id_level3=1041, leve3='Квест'),\n",
       " Row(id_level3=1043, leve3='Брейн-ринг'),\n",
       " Row(id_level3=323, leve3='Современные настольные игры')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = spark.read.csv('./groups.csv', header='true', inferSchema = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('уникальный номер', 'int'),\n",
       " ('направление 1', 'string'),\n",
       " ('направление 2', 'string'),\n",
       " ('направление 3', 'string'),\n",
       " ('адрес площадки', 'string'),\n",
       " ('округ площадки', 'string'),\n",
       " ('район площадки', 'string'),\n",
       " ('расписание в активных периодах', 'string'),\n",
       " ('расписание в закрытых периодах', 'string'),\n",
       " ('расписание в плановом периоде', 'string')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = groups.drop('направление 1') \\\n",
    "                        .drop('направление 2') \\\n",
    "                        .drop('адрес площадки') \\\n",
    "                        .drop('округ площадки') \\\n",
    "                        .drop('район площадки') \\\n",
    "                        .drop('расписание в активных периодах') \\\n",
    "                        .drop('расписание в закрытых периодах') \\\n",
    "                        .drop('расписание в плановом периоде') \\\n",
    "                        .withColumnRenamed('уникальный номер', 'groupId') \\\n",
    "                        .withColumnRenamed('направление 3', 'leve3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(groupId=801357270, leve3='ОФП'),\n",
       " Row(groupId=801356857, leve3='ОФП'),\n",
       " Row(groupId=801351684, leve3='ОФП'),\n",
       " Row(groupId=801353683, leve3='ОФП'),\n",
       " Row(groupId=801352164, leve3='ОФП')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(leve3='ОФП', id_level3=104, groupId=801357270),\n",
       " Row(leve3='ОФП', id_level3=104, groupId=801356857),\n",
       " Row(leve3='ОФП', id_level3=104, groupId=801351684),\n",
       " Row(leve3='ОФП', id_level3=104, groupId=801353683),\n",
       " Row(leve3='ОФП', id_level3=104, groupId=801352164)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupsWithLevelIds = categories.join(groups, 'leve3')\n",
    "groupsWithLevelIds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(groupId=801346710, userId=101386726, rating=1, leve3='Дыхательная гимнастика', id_level3=171),\n",
       " Row(groupId=801346710, userId=101430794, rating=1, leve3='Дыхательная гимнастика', id_level3=171),\n",
       " Row(groupId=801346810, userId=101366986, rating=1, leve3='ОНЛАЙН Ментальная арифметика', id_level3=1173),\n",
       " Row(groupId=801346810, userId=101374816, rating=1, leve3='ОНЛАЙН Ментальная арифметика', id_level3=1173),\n",
       " Row(groupId=801346810, userId=101381146, rating=1, leve3='ОНЛАЙН Ментальная арифметика', id_level3=1173)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attendanceWithLevelIds=attendance.join(groupsWithLevelIds, 'groupId')\n",
    "attendanceWithLevelIds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6537789"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attendanceWithLevelIds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userId=101419598, id_level3=1788, sum(rating)=20),\n",
       " Row(userId=101357168, id_level3=1113, sum(rating)=26),\n",
       " Row(userId=101364367, id_level3=1421, sum(rating)=71),\n",
       " Row(userId=101390957, id_level3=1165, sum(rating)=6),\n",
       " Row(userId=101365143, id_level3=1421, sum(rating)=35)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interests = attendanceWithLevelIds.groupBy(\"userId\", \"id_level3\").agg({\"rating\": \"sum\"})\n",
    "interests.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "316550"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interests.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "# split into \n",
    "(trainingInterests, testInterests) = interests.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "alsInterests = ALS(maxIter=5,rank=4, regParam=0.01, userCol=\"userId\", itemCol=\"id_level3\", ratingCol=\"sum(rating)\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# fit the ALS model to the training set\n",
    "modelInterests = alsInterests.fit(trainingInterests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=101346610, features=[25.293867111206055, -39.14100646972656, 18.81976318359375, 64.5489273071289])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelInterests.userFactors.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9693:==============>                                         (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 57.550114179685835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictionsInterests = modelInterests.transform(testInterests)\n",
    "evaluatorInterests = RegressionEvaluator(metricName=\"rmse\", labelCol=\"sum(rating)\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmseInterests = evaluatorInterests.evaluate(predictionsInterests)\n",
    "print(\"Root-mean-square error = \" + str(rmseInterests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/26 00:59:28 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/26 00:59:28 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                ]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_507da242d782, rank=50"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "als_modelInterests =  ALS(userCol=\"userId\", itemCol=\"id_level3\", ratingCol=\"sum(rating)\", coldStartStrategy=\"drop\")\n",
    "\n",
    "                 \n",
    "paramsInterests = ParamGridBuilder().addGrid(als_modelInterests.regParam, [0.01,0.001,0.1]).addGrid(als_modelInterests.rank, [4,10,50]).build()\n",
    "\n",
    "\n",
    "## instantiating crossvalidator estimator\n",
    "cvInterests = CrossValidator(estimator=als_modelInterests, estimatorParamMaps=paramsInterests,evaluator=evaluatorInterests,parallelism=4)\n",
    "best_modelInterests = cvInterests.fit(interests)    \n",
    "\n",
    "# We see the best model has a rank of 50, so we will use that in our future models with this dataset\n",
    "best_modelInterests.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "# split into \n",
    "(trainingInterests, testInterests) = interests.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "alsInterests = ALS(maxIter=5,rank=100, regParam=0.01, userCol=\"userId\", itemCol=\"id_level3\", ratingCol=\"sum(rating)\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# fit the ALS model to the training set\n",
    "modelInterests = alsInterests.fit(trainingInterests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10546:================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 28.143804777455088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictionsInterests = modelInterests.transform(testInterests)\n",
    "evaluatorInterests = RegressionEvaluator(metricName=\"rmse\", labelCol=\"sum(rating)\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmseInterests = evaluatorInterests.evaluate(predictionsInterests)\n",
    "print(\"Root-mean-square error = \" + str(rmseInterests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userId=101419598, recommendations=[Row(id_level3=448, rating=58.27103042602539), Row(id_level3=149, rating=54.70125198364258), Row(id_level3=1720, rating=53.996788024902344), Row(id_level3=1118, rating=47.52177047729492), Row(id_level3=152, rating=45.99745178222656), Row(id_level3=1421, rating=45.98931121826172), Row(id_level3=1416, rating=45.182411193847656), Row(id_level3=656, rating=41.04288101196289), Row(id_level3=1280, rating=37.36673355102539), Row(id_level3=1287, rating=37.02436065673828)])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendationsInterests = modelInterests.recommendForAllUsers(10)\n",
    "recommendationsInterests.where(recommendationsInterests[\"userId\"] == 101419598).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
